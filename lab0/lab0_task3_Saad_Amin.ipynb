{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadam1n/CS4803-EML/blob/main/lab0/lab0_task3_Saad_Amin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pleas read:\n",
        "\n",
        "before your start running this jupyter notebook, please click Edit > Notebook Settings and choose any of the available GPUs."
      ],
      "metadata": {
        "id": "P-UzeCP8doGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Import Python Packages"
      ],
      "metadata": {
        "id": "pPj69Ai0VJpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install thop if not install already:"
      ],
      "metadata": {
        "id": "SjnPjKcSCLXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6vMy9FqCCuc",
        "outputId": "c0f658d7-d71e-4aa8-9c5e-48fa189fbfc3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.12/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from thop) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->thop) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->thop) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all packages"
      ],
      "metadata": {
        "id": "6vw9Og9CCOj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SEsUoci9Uv5x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## TODO: you may import more packages below:\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.profiler import profile, ProfilerActivity, record_function\n",
        "import torch.cuda.profiler as profiler\n",
        "\n",
        "from thop import profile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Define your first network"
      ],
      "metadata": {
        "id": "jTjEJu-1VHvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet Block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.silu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = out + self.shortcut(x)\n",
        "        out = F.silu(out)\n",
        "        return out\n",
        "\n",
        "# MobileNetV2 block\n",
        "class MobileNetV2Block(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(MobileNetV2Block, self).__init__()\n",
        "\n",
        "        expansion_ratio = 4\n",
        "        mid_planes = planes * expansion_ratio\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, mid_planes, kernel_size=1, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_planes, mid_planes, groups=mid_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_planes, planes, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(planes)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x) + self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# https://arxiv.org/abs/2111.11418\n",
        "class PoolFormerBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PoolFormerBlock, self).__init__()\n",
        "\n",
        "        self.pooler = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_planes),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=stride, padding=1)\n",
        "        )\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_planes),\n",
        "            nn.Conv2d(in_planes, planes * 2, kernel_size=1, stride=1, bias=False),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(planes * 2, planes, kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.BatchNorm2d(in_planes),\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pooler(x) + x\n",
        "        out = self.ffn(out) + self.shortcut(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet Model\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # we utilize lazy in places to make it compatible with both CIFAR and MNIST\n",
        "\n",
        "        self.conv1 = nn.LazyConv2d(64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.LazyLinear(num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "metadata": {
        "id": "doptrjrMVGDl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implement training loop and test function"
      ],
      "metadata": {
        "id": "jDG9sEPOXjYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I put in quite a bit of extra stuff to profile/optimizing training\n",
        "# since I have an interview coming where I need to do that\n",
        "\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "\n",
        "    # Compile for training\n",
        "    model.compile(mode=\"reduce-overhead\")\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_curve = []\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "    print_acc = False\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(epoch)):\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          data, target = data.to(device), target.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "              output = model(data)\n",
        "              loss = F.cross_entropy(output, target)\n",
        "\n",
        "          scaler.scale(loss).backward()\n",
        "\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          if batch_idx % 100 == 0:\n",
        "              loss_curve.append(loss.item())\n",
        "\n",
        "              if print_acc:\n",
        "                _, predicted = output.max(1)\n",
        "                total += target.size(0)\n",
        "                correct += predicted.eq(target).sum().item()\n",
        "\n",
        "                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    return loss_curve\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    # TODO: finish this function, you are required to:\n",
        "    # 1. print the loss and classification accuracy on the test set\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      # This should force recompilation from training... I think\n",
        "      model.compile(mode=\"reduce-overhead\")\n",
        "\n",
        "      running_loss = 0.0\n",
        "\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      total_latency = 0.0\n",
        "      macs = 0\n",
        "\n",
        "      for batch_idx, (data, target) in enumerate(test_loader):\n",
        "          data, target = data.to(device), target.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          start = torch.cuda.Event(enable_timing=True)\n",
        "          end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "          with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "            # torch.compile warmup before collecting timing data\n",
        "            if batch_idx == 0:\n",
        "              print(\"Warming up for torch.compile()...\")\n",
        "              for _ in range(3):\n",
        "                  _ = model(data)\n",
        "\n",
        "            start.record()\n",
        "            output = model(data)\n",
        "            end.record()\n",
        "\n",
        "            loss = F.cross_entropy(output, target)\n",
        "\n",
        "            if batch_idx == 0:\n",
        "              # get a batch size of 1\n",
        "              macs, params = profile(model, inputs=(data[:1], ))\n",
        "\n",
        "          _, predicted = output.max(1)\n",
        "          total += target.size(0)\n",
        "          correct += predicted.eq(target).sum().item()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          # Measure latency\n",
        "          torch.cuda.synchronize()\n",
        "          total_latency += start.elapsed_time(end)\n",
        "\n",
        "      print(f'Loss: {running_loss / len(test_loader):.4f}, Acc: {100.*correct/total:.2f}%, Latency: {total_latency / len(test_loader):.2f}ms, MACs: {macs / 1000000:.1f}M')\n"
      ],
      "metadata": {
        "id": "Qk-kH1KwXfG3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Dataset loading\n"
      ],
      "metadata": {
        "id": "j_rWlLGFXyIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: finish the code here, you are required to:\n",
        "# 1. load the training and testing split of the MNIST dataset\n",
        "\n",
        "load_mnist = False\n",
        "\n",
        "if load_mnist:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "else:\n",
        "    # Data transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    # Dataset and DataLoader\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                 download=True, transform=transform_train)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                                download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=384, shuffle=True, pin_memory=True)\n",
        "\n",
        "    # assume that 8 is what we would have in a real inference scenario\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Smfbu4RXuUd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Put Everything Together"
      ],
      "metadata": {
        "id": "Wzqkx8TTYDxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collected using `torch.compile(mode=\"default\")` on a A100 system:\n",
        "\n",
        "Regular ResNet:\n",
        "- MNIST: Loss: 0.0178, Acc: 99.54%, Latency: 2.14ms, MACs: 457.7M\n",
        "- CIFAR: Loss: 0.4036, Acc: 87.25%, Latency: 1.97ms, MACs: 557.9M\n",
        "\n",
        "MobileNetV2:\n",
        "- MNIST: Loss: 0.0181, Acc: 99.54%, Latency: 2.86ms, MACs: 269.7M\n",
        "- CIFAR: Loss: 0.3543, Acc: 88.74%, Latency: 2.36ms, MACs: 337.2M\n",
        "\n",
        "Given that we are seing negliglbe latency difference with reducing MACs, there are two possible bottlenecks:\n",
        "- We are kernel launch bound. This is a very likely factor since we are performing computations on very small tensors and batch sizes. We might be able to solve this with `torch.compile(\"reduce-overhead\")` which will create CUDA graphs to launch kernels sequentially from the GPU instead of waiting for the CPU to issue new kernel launch operations.\n",
        "- We are memory bound. This is also a likely possibility since reducing MACs doesn't appear to affect latency, albiet less likely since kernel launch overhead is probably the biggest factor here. Given this limitation, we might want to come up with architectures that increase arithmetic intensity. ViTs would fit this requirement, but are out of the scope of this HW. Another possibility is to increase the number of pointwise and fully connected convolutions, but the original ResNet does that already.\n",
        "\n",
        "When we utilize `reduce-overhead` (all data besides latency and MNIST omitted):\n",
        "\n",
        "Regular ResNet:\n",
        "- CIFAR: Latency: 0.71ms\n",
        "\n",
        "MobileNetV2:\n",
        "- CIFAR: Latency: 0.77ms\n",
        "\n",
        "Damn it! We're still CPU bound!\n"
      ],
      "metadata": {
        "id": "1xWwmJXh-G5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: finish the code here, you are required to:\n",
        "# 1. launch model training: recommended hyperparameter: batch size = 64, learning rate = 1.0, adam optimizer\n",
        "# 2. visualize the training loss curve using matplotlib or any other libraries\n",
        "\n",
        "latency_test_only = True\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = ResNet18()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "if not latency_test_only:\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
        "\n",
        "    losses = train(model, device, train_loader, optimizer, scheduler, 16)\n",
        "\n",
        "    steps = [i * 100 for i in range(len(losses))]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(steps, losses, marker='o', label=\"Training Loss\")\n",
        "\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# run test\n",
        "test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "WHr46bWLYHy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d89739-3932-4b31-8de2-f26f99a41357"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warming up for torch.compile()...\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "skipping cudagraphs due to skipping cudagraphs due to cpu device (arg2_1). Found from : \n",
            "   File \"/tmp/ipython-input-1519489046.py\", line 109, in forward\n",
            "    out = F.relu(self.bn1(self.conv1(x)))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thop/vision/basic_hooks.py\", line 28, in count_convNd\n",
            "    m.total_ops += calculate_conv2d_flops(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.3026, Acc: 10.00%, Latency: 0.71ms, MACs: 557.9M\n"
          ]
        }
      ]
    }
  ]
}